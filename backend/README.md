# SimplyLegal Backend

A FastAPI-based backend that translates complex legal documents to plain language and generates audio narration.

## âœ¨ Features

- ğŸ“„ **Legal Text Translation**: Converts complex legal jargon to plain English
- ğŸ”Š **Text-to-Speech**: Generates audio narration of translated text
- ğŸ—‚ï¸ **Smart Chunking**: Intelligently splits long documents for efficient processing
- âš ï¸ **Error Handling**: Comprehensive validation and error recovery
- ğŸ“Š **Monitoring**: Health check endpoint and detailed logging
- âš™ï¸ **Configurable**: Environment-based configuration for all settings

## ğŸš€ Quick Start

```bash
# Copy environment template
cp .env.example .env

# Install dependencies
pip install -r requirements.txt

# Run the server
uvicorn main:app --reload
```

Server runs on: `http://localhost:8000`

## ğŸ“– Documentation

**All documentation is in the `/docs` folder:**

- **[docs/IMPLEMENTATION_SUMMARY.md](docs/IMPLEMENTATION_SUMMARY.md)** - Complete project overview
- **[docs/INTEGRATION_GUIDE.md](docs/INTEGRATION_GUIDE.md)** - Frontend integration guide
- **[docs/CHUNKING.md](docs/CHUNKING.md)** - Text chunking documentation

ğŸ‘‰ **Start here**: [docs/README.md](docs/README.md)

## ğŸ§ª Test the API

```bash
# Health check
curl http://localhost:8000/api/health

# Translate legal text
curl -X POST http://localhost:8000/api/llm_output \
  -H "Content-Type: application/json" \
  -d '{"text":"This deed of conveyance is hereby executed..."}'
```

## ğŸ“‹ Requirements

- Python 3.8+
- Ollama (for LLM processing)
- pyttsx3 (for text-to-speech)

See `requirements.txt` for full dependency list.

## ğŸ”§ Configuration

All configuration is done via `.env` file. See `.env.example` for available options:

```env
# LLM Settings
LLM_MODEL=deepseek-r1:8b
LLM_BASE_URL=http://localhost:11434

# Chunking Settings
CHUNK_SIZE=1000
ENABLE_CHUNKING=true
```

## ğŸ“š Project Structure

```
backend/
â”œâ”€â”€ main.py                    # FastAPI application
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env.example              # Configuration template
â”œâ”€â”€ test_chunking.py          # Chunking tests
â”œâ”€â”€ models/                   # Request/response models
â”œâ”€â”€ routers/                  # API endpoints
â”œâ”€â”€ services/                 # Business logic
â”‚   â”œâ”€â”€ llm_tts.py           # LLM & TTS service
â”‚   â””â”€â”€ text_chunker.py      # Text chunking
â””â”€â”€ docs/                     # ğŸ“š Documentation
    â”œâ”€â”€ README.md            # Doc index
    â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md
    â”œâ”€â”€ INTEGRATION_GUIDE.md
    â””â”€â”€ CHUNKING.md
```

## ğŸ¯ API Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/health` | GET | Health check |
| `/api/` | GET | Welcome message |
| `/api/llm_output` | POST | Translate legal text to plain language with audio |

## ğŸ“Š Status

âœ… **Production Ready** for hackathon deployment

**Implemented:**
- Environment-based configuration
- Comprehensive error handling
- Intelligent text chunking
- Complete logging
- Input validation
- API monitoring

## ğŸ†˜ Troubleshooting

Check the troubleshooting section in [docs/INTEGRATION_GUIDE.md](docs/INTEGRATION_GUIDE.md).

Common issues:
- **"Cannot connect to LLM"**: Ensure Ollama is running
- **Slow responses**: Adjust CHUNK_SIZE or increase timeout
- **Empty input error**: Ensure text is 1-5000 characters

## ğŸ“ License

[Add your license here]

## ğŸ‘¥ Team

Built for the accessibility hackathon ğŸ¯

---

**For detailed documentation, see [docs/README.md](docs/README.md)**
